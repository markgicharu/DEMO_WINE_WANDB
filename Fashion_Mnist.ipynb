{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets\r\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:49, 530177.95it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 201593.68it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:11, 386595.28it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\markg\\venv\\torch-venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "training_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=True,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "test_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=False,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits\r\n",
    "\r\n",
    "model = NeuralNetwork()\r\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\r\n",
    "batch_size = 64\r\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    for batch, (X, y) in enumerate(dataloader):\r\n",
    "        # Compute prediction and loss\r\n",
    "        pred = model(X)\r\n",
    "        loss = loss_fn(pred, y)\r\n",
    "\r\n",
    "        # Backpropagation\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if batch % 100 == 0:\r\n",
    "            loss, current = loss.item(), batch * len(X)\r\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\r\n",
    "\r\n",
    "\r\n",
    "def test_loop(dataloader, model, loss_fn):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    num_batches = len(dataloader)\r\n",
    "    test_loss, correct = 0, 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            pred = model(X)\r\n",
    "            test_loss += loss_fn(pred, y).item()\r\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n",
    "\r\n",
    "    test_loss /= num_batches\r\n",
    "    correct /= size\r\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306147  [    0/60000]\n",
      "loss: 2.302284  [ 6400/60000]\n",
      "loss: 2.290239  [12800/60000]\n",
      "loss: 2.286166  [19200/60000]\n",
      "loss: 2.296227  [25600/60000]\n",
      "loss: 2.283005  [32000/60000]\n",
      "loss: 2.273202  [38400/60000]\n",
      "loss: 2.268317  [44800/60000]\n",
      "loss: 2.264401  [51200/60000]\n",
      "loss: 2.269440  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.251235 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.247412  [    0/60000]\n",
      "loss: 2.257284  [ 6400/60000]\n",
      "loss: 2.218485  [12800/60000]\n",
      "loss: 2.223961  [19200/60000]\n",
      "loss: 2.251416  [25600/60000]\n",
      "loss: 2.235266  [32000/60000]\n",
      "loss: 2.215973  [38400/60000]\n",
      "loss: 2.203359  [44800/60000]\n",
      "loss: 2.197989  [51200/60000]\n",
      "loss: 2.220419  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.178470 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.177101  [    0/60000]\n",
      "loss: 2.195122  [ 6400/60000]\n",
      "loss: 2.117975  [12800/60000]\n",
      "loss: 2.126375  [19200/60000]\n",
      "loss: 2.187031  [25600/60000]\n",
      "loss: 2.168182  [32000/60000]\n",
      "loss: 2.131276  [38400/60000]\n",
      "loss: 2.109359  [44800/60000]\n",
      "loss: 2.102832  [51200/60000]\n",
      "loss: 2.151536  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 2.075550 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.082099  [    0/60000]\n",
      "loss: 2.111415  [ 6400/60000]\n",
      "loss: 1.982043  [12800/60000]\n",
      "loss: 1.997489  [19200/60000]\n",
      "loss: 2.114788  [25600/60000]\n",
      "loss: 2.085093  [32000/60000]\n",
      "loss: 2.033135  [38400/60000]\n",
      "loss: 2.003318  [44800/60000]\n",
      "loss: 2.004970  [51200/60000]\n",
      "loss: 2.084331  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 1.974716 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.984521  [    0/60000]\n",
      "loss: 2.028136  [ 6400/60000]\n",
      "loss: 1.854505  [12800/60000]\n",
      "loss: 1.887616  [19200/60000]\n",
      "loss: 2.054612  [25600/60000]\n",
      "loss: 2.012953  [32000/60000]\n",
      "loss: 1.954934  [38400/60000]\n",
      "loss: 1.922886  [44800/60000]\n",
      "loss: 1.927424  [51200/60000]\n",
      "loss: 2.035860  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.897716 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.904252  [    0/60000]\n",
      "loss: 1.961101  [ 6400/60000]\n",
      "loss: 1.758000  [12800/60000]\n",
      "loss: 1.806004  [19200/60000]\n",
      "loss: 1.997270  [25600/60000]\n",
      "loss: 1.954383  [32000/60000]\n",
      "loss: 1.896441  [38400/60000]\n",
      "loss: 1.862945  [44800/60000]\n",
      "loss: 1.864923  [51200/60000]\n",
      "loss: 1.996230  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 1.835822 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.838123  [    0/60000]\n",
      "loss: 1.907777  [ 6400/60000]\n",
      "loss: 1.682633  [12800/60000]\n",
      "loss: 1.738167  [19200/60000]\n",
      "loss: 1.942236  [25600/60000]\n",
      "loss: 1.903430  [32000/60000]\n",
      "loss: 1.848332  [38400/60000]\n",
      "loss: 1.814576  [44800/60000]\n",
      "loss: 1.813148  [51200/60000]\n",
      "loss: 1.959818  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 1.783060 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.780799  [    0/60000]\n",
      "loss: 1.862311  [ 6400/60000]\n",
      "loss: 1.619941  [12800/60000]\n",
      "loss: 1.677935  [19200/60000]\n",
      "loss: 1.892070  [25600/60000]\n",
      "loss: 1.856960  [32000/60000]\n",
      "loss: 1.807046  [38400/60000]\n",
      "loss: 1.774629  [44800/60000]\n",
      "loss: 1.769101  [51200/60000]\n",
      "loss: 1.925928  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.738080 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.731333  [    0/60000]\n",
      "loss: 1.821287  [ 6400/60000]\n",
      "loss: 1.567126  [12800/60000]\n",
      "loss: 1.625308  [19200/60000]\n",
      "loss: 1.849325  [25600/60000]\n",
      "loss: 1.816645  [32000/60000]\n",
      "loss: 1.773090  [38400/60000]\n",
      "loss: 1.742880  [44800/60000]\n",
      "loss: 1.731846  [51200/60000]\n",
      "loss: 1.897343  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.701254 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.689578  [    0/60000]\n",
      "loss: 1.787260  [ 6400/60000]\n",
      "loss: 1.524002  [12800/60000]\n",
      "loss: 1.583448  [19200/60000]\n",
      "loss: 1.814988  [25600/60000]\n",
      "loss: 1.783886  [32000/60000]\n",
      "loss: 1.745865  [38400/60000]\n",
      "loss: 1.718162  [44800/60000]\n",
      "loss: 1.701397  [51200/60000]\n",
      "loss: 1.873416  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 1.671824 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "for t in range(epochs):\r\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\r\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\r\n",
    "    test_loop(test_dataloader, model, loss_fn)\r\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "778de6fd147bda85163a324ce7339acfc1c656b45f8b8593316159881a238432"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('torch-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}