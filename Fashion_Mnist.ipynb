{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets\r\n",
    "from torchvision.transforms import ToTensor, Lambda\r\n",
    "from torch import optim\r\n",
    "\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "training_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=True,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "test_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=False,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR NEURAL NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits\r\n",
    "\r\n",
    "model = NeuralNetwork()\r\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CONVOLUTION NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\r\n",
    "    def __init__(self, input_shape=(1,28,28)):\r\n",
    "        super(ConvNet, self).__init__()\r\n",
    "\r\n",
    "        # DEFINE THE CONVOLUTION LAYERS\r\n",
    "\r\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\r\n",
    "        self.conv2 = nn.Conv2d(32, 64,3)\r\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\r\n",
    "\r\n",
    "        # DEFINE THE POOLING LATER\r\n",
    "\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "\r\n",
    "        n_size = self._get_conv_output(input_shape)\r\n",
    "\r\n",
    "        # DEFINE THE LINEAR CLASSS\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(n_size, 512)\r\n",
    "        self.fc2 = nn.Linear(512, 10)\r\n",
    "\r\n",
    "    def _get_conv_output(self, shape):\r\n",
    "        batch_size = 1\r\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\r\n",
    "        output_feat = self._forward_features(input)\r\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\r\n",
    "        return n_size\r\n",
    "\r\n",
    "    def _forward_features(self, X):\r\n",
    "        X = self.pool(F.relu(self.conv1(X)))\r\n",
    "        X = self.pool(F.relu(self.conv2(X)))\r\n",
    "        X = self.pool(F.relu(self.conv3(X)))\r\n",
    "        return X\r\n",
    "\r\n",
    "    def forward(self, X):\r\n",
    "        X = self._forward_features(X)\r\n",
    "        X = X.view(X.size(0), -1)\r\n",
    "        X = F.relu(self.fc1(X))\r\n",
    "        X = self.fc2(X)\r\n",
    "        return X\r\n",
    "model = ConvNet()\r\n",
    "model.to(device)\r\n",
    "print(model)\r\n",
    "\r\n",
    "#loss = nn.CrossEntropyLoss()\r\n",
    "#optimizer = optim.Adam(net.parameters())\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\r\n",
    "batch_size = 64\r\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    for batch, (X, y) in enumerate(dataloader):\r\n",
    "        X, y = X.to(device), y.to(device)\r\n",
    "        # Compute prediction and loss\r\n",
    "        pred = model(X)\r\n",
    "        loss = loss_fn(pred, y)\r\n",
    "\r\n",
    "        # Backpropagation\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if batch % 100 == 0:\r\n",
    "            loss, current = loss.item(), batch * len(X)\r\n",
    "            print(f\"loss: {loss:>3f}  [{current:>5d}/{size:>5d}]\")\r\n",
    "\r\n",
    "\r\n",
    "def test_loop(dataloader, model, loss_fn):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    num_batches = len(dataloader)\r\n",
    "    test_loss, correct = 0, 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            X, y = X.to(device), y.to(device)\r\n",
    "            pred = model(X)\r\n",
    "            test_loss += loss_fn(pred, y).item()\r\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n",
    "\r\n",
    "    test_loss /= num_batches\r\n",
    "    correct /= size\r\n",
    "    print(f\"Test Metrics: \\n Accuracy: {(100*correct):>0.1f}%, Test loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -364631719936.000000  [    0/60000]\n",
      "loss: -441673187328.000000  [ 6400/60000]\n",
      "loss: -625939382272.000000  [12800/60000]\n",
      "loss: -777666625536.000000  [19200/60000]\n",
      "loss: -1105881989120.000000  [25600/60000]\n",
      "loss: -1499634860032.000000  [32000/60000]\n",
      "loss: -1868715917312.000000  [38400/60000]\n",
      "loss: -2555400945664.000000  [44800/60000]\n",
      "loss: -3542899490816.000000  [51200/60000]\n",
      "loss: -4521844539392.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -5117620313538.038086 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -5102444216320.000000  [    0/60000]\n",
      "loss: -6239224332288.000000  [ 6400/60000]\n",
      "loss: -8753348870144.000000  [12800/60000]\n",
      "loss: -10559533613056.000000  [19200/60000]\n",
      "loss: -14323070009344.000000  [25600/60000]\n",
      "loss: -18273336819712.000000  [32000/60000]\n",
      "loss: -21179114979328.000000  [38400/60000]\n",
      "loss: -26728376303616.000000  [44800/60000]\n",
      "loss: -34036579827712.000000  [51200/60000]\n",
      "loss: -39802153992192.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -43560207738553.882812 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -43429090295808.000000  [    0/60000]\n",
      "loss: -48665636896768.000000  [ 6400/60000]\n",
      "loss: -62690709995520.000000  [12800/60000]\n",
      "loss: -69662788288512.000000  [19200/60000]\n",
      "loss: -87292601434112.000000  [25600/60000]\n",
      "loss: -103313903190016.000000  [32000/60000]\n",
      "loss: -111492762435584.000000  [38400/60000]\n",
      "loss: -131561659826176.000000  [44800/60000]\n",
      "loss: -157260093325312.000000  [51200/60000]\n",
      "loss: -173250072018944.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -185525298640126.375000 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -184962129068032.000000  [    0/60000]\n",
      "loss: -196241719820288.000000  [ 6400/60000]\n",
      "loss: -240153482231808.000000  [12800/60000]\n",
      "loss: -254364723707904.000000  [19200/60000]\n",
      "loss: -304605455974400.000000  [25600/60000]\n",
      "loss: -345516395200512.000000  [32000/60000]\n",
      "loss: -358196413726720.000000  [38400/60000]\n",
      "loss: -407041466171392.000000  [44800/60000]\n",
      "loss: -469562097139712.000000  [51200/60000]\n",
      "loss: -500204776194048.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -529093151748435.187500 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -527480133779456.000000  [    0/60000]\n",
      "loss: -542553959038976.000000  [ 6400/60000]\n",
      "loss: -644773039833088.000000  [12800/60000]\n",
      "loss: -664327421952000.000000  [19200/60000]\n",
      "loss: -774880215695360.000000  [25600/60000]\n",
      "loss: -857362579587072.000000  [32000/60000]\n",
      "loss: -868001146470400.000000  [38400/60000]\n",
      "loss: -964466044829696.000000  [44800/60000]\n",
      "loss: -1089077877145600.000000  [51200/60000]\n",
      "loss: -1136714366058496.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -1193355643737290.250000 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -1189708088475648.000000  [    0/60000]\n",
      "loss: -1200595964788736.000000  [ 6400/60000]\n",
      "loss: -1401085440819200.000000  [12800/60000]\n",
      "loss: -1418852109910016.000000  [19200/60000]\n",
      "loss: -1627715564208128.000000  [25600/60000]\n",
      "loss: -1772699298824192.000000  [32000/60000]\n",
      "loss: -1767598488289280.000000  [38400/60000]\n",
      "loss: -1935719748599808.000000  [44800/60000]\n",
      "loss: -2155564360531968.000000  [51200/60000]\n",
      "loss: -2219871563677696.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -2318928588927941.500000 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -2311828960968704.000000  [    0/60000]\n",
      "loss: -2303623157514240.000000  [ 6400/60000]\n",
      "loss: -2655799469932544.000000  [12800/60000]\n",
      "loss: -2658340211523584.000000  [19200/60000]\n",
      "loss: -3015483385184256.000000  [25600/60000]\n",
      "loss: -3248745474949120.000000  [32000/60000]\n",
      "loss: -3205671784808448.000000  [38400/60000]\n",
      "loss: -3475437338492928.000000  [44800/60000]\n",
      "loss: -3832742647169024.000000  [51200/60000]\n",
      "loss: -3910131783827456.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -4070333101362130.500000 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -4057857183973376.000000  [    0/60000]\n",
      "loss: -4007396418519040.000000  [ 6400/60000]\n",
      "loss: -4580240634413056.000000  [12800/60000]\n",
      "loss: -4546590639390720.000000  [19200/60000]\n",
      "loss: -5115769906003968.000000  [25600/60000]\n",
      "loss: -5468554761601024.000000  [32000/60000]\n",
      "loss: -5355147223891968.000000  [38400/60000]\n",
      "loss: -5763284007387136.000000  [44800/60000]\n",
      "loss: -6310623365300224.000000  [51200/60000]\n",
      "loss: -6393506570436608.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -6638210523178995.000000 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -6617845597208576.000000  [    0/60000]\n",
      "loss: -6492203845156864.000000  [ 6400/60000]\n",
      "loss: -7372476719824896.000000  [12800/60000]\n",
      "loss: -7272757779759104.000000  [19200/60000]\n",
      "loss: -8133416075657216.000000  [25600/60000]\n",
      "loss: -8643021998391296.000000  [32000/60000]\n",
      "loss: -8415038154997760.000000  [38400/60000]\n",
      "loss: -9005778157436928.000000  [44800/60000]\n",
      "loss: -9807373136822272.000000  [51200/60000]\n",
      "loss: -9883330707193856.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -10241116727348908.000000 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -10209676280987648.000000  [    0/60000]\n",
      "loss: -9964498072895488.000000  [ 6400/60000]\n",
      "loss: -11259086041513984.000000  [12800/60000]\n",
      "loss: -11052980224655360.000000  [19200/60000]\n",
      "loss: -12302216559853568.000000  [25600/60000]\n",
      "loss: -13012606298095616.000000  [32000/60000]\n",
      "loss: -12611990002335744.000000  [38400/60000]\n",
      "loss: -13437862821232640.000000  [44800/60000]\n",
      "loss: -14570913848623104.000000  [51200/60000]\n",
      "loss: -14621772703858688.000000  [57600/60000]\n",
      "Test Metrics: \n",
      " Accuracy: 10.0%, Test loss: -15127035816233520.000000 \n",
      "\n",
      "Done!\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# define loss and optimizer\r\n",
    "#loss_fn = nn.CrossEntropyLoss()\r\n",
    "loss_fn = nn.NLLLoss()\r\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "for t in range(epochs):\r\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\r\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\r\n",
    "    test_loop(test_dataloader, model, loss_fn)\r\n",
    "print(\"Done!\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "778de6fd147bda85163a324ce7339acfc1c656b45f8b8593316159881a238432"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('torch-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}