{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets\r\n",
    "from torchvision.transforms import ToTensor, Lambda\r\n",
    "from torch import optim\r\n",
    "import torchmetrics\r\n",
    "\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: markgich (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\r\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "training_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=True,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "test_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=False,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.25),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.25),\r\n",
    "            nn.Linear(512, 10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits\r\n",
    "\r\n",
    "model = NeuralNetwork()\r\n",
    "model.to(device)\r\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CONVOLUTION NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ConvNet(nn.Module):\r\n",
    "    def __init__(self, input_shape=(1,28,28)):\r\n",
    "        super(ConvNet, self).__init__()\r\n",
    "\r\n",
    "        # DEFINE THE CONVOLUTION LAYERS\r\n",
    "\r\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\r\n",
    "        self.conv2 = nn.Conv2d(32, 64,3)\r\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\r\n",
    "\r\n",
    "        # DEFINE THE POOLING LATER\r\n",
    "\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "\r\n",
    "        n_size = self._get_conv_output(input_shape)\r\n",
    "\r\n",
    "        # DEFINE THE LINEAR CLASSS\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(n_size, 512)\r\n",
    "        self.fc2 = nn.Linear(512, 10)\r\n",
    "\r\n",
    "        # DEFINE THE DROPOUT\r\n",
    "        self.dropout = nn.Dropout(p=0.25)\r\n",
    "\r\n",
    "    def _get_conv_output(self, shape):\r\n",
    "        batch_size = 1\r\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\r\n",
    "        output_feat = self._forward_features(input)\r\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\r\n",
    "        return n_size\r\n",
    "\r\n",
    "    def _forward_features(self, X):\r\n",
    "        X = self.pool(F.relu(self.conv1(X)))\r\n",
    "        X = self.pool(F.relu(self.conv2(X)))\r\n",
    "        X = self.pool(F.relu(self.conv3(X)))\r\n",
    "        return X\r\n",
    "\r\n",
    "    def forward(self, X):\r\n",
    "        X = self._forward_features(X)\r\n",
    "        X = X.view(X.size(0), -1)\r\n",
    "        X = self.dropout(X) # DROPOUT\r\n",
    "        X = F.relu(self.fc1(X))\r\n",
    "        X = self.dropout(X) # DROPOUT\r\n",
    "        X = self.fc2(X)\r\n",
    "        return X\r\n",
    "        \r\n",
    "model = ConvNet()\r\n",
    "\r\n",
    "# PUSH MODEL TO DEVICE(CUDA)\r\n",
    "model.to(device)\r\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(learning_rate = 1e-3,\r\n",
    "                batch_size = 64,\r\n",
    "                epochs = 10,\r\n",
    "                loss = nn.NLLLoss(),\r\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.001\n",
      "batch_size: 64\n",
      "epochs: 10\n"
     ]
    }
   ],
   "source": [
    "for k, v in config.items():\r\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    #acc = 0\r\n",
    "    for batch, (X, y) in enumerate(dataloader):\r\n",
    "        X, y = X.to(device), y.to(device)\r\n",
    "        # Compute prediction and loss\r\n",
    "        pred = model(X)\r\n",
    "        loss = loss_fn(pred, y)\r\n",
    "        acc = torchmetrics.functional.accuracy(pred, y)\r\n",
    "\r\n",
    "        # Backpropagation\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if batch % 100 == 0:\r\n",
    "            loss, current = loss.item(), batch * len(X)\r\n",
    "            print(f\"train loss: {loss:>3f}  [{current:>5d}/{size:>5d}]\")\r\n",
    "            #wandb.log({\"train loss\": loss, \"train accuracy\": acc})\r\n",
    "    print(f\"Train Metrics: \\n Train Accuracy: {(100*acc):>0.1f}%, Train Loss: {loss:>8f} \\n\")\r\n",
    "    wandb.log({\"Train Accuracy\": acc, \"Train Loss\": loss })\r\n",
    "\r\n",
    "\r\n",
    "def test_loop(dataloader, model, loss_fn):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    num_batches = len(dataloader)\r\n",
    "    test_loss, correct = 0, 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            X, y = X.to(device), y.to(device)\r\n",
    "            pred = model(X)\r\n",
    "            test_loss += loss_fn(pred, y).item()\r\n",
    "            acc = torchmetrics.functional.accuracy(pred, y).item()\r\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n",
    "    wandb.log({\"Test Accuracy\":acc, \"Test Loss\": test_loss})\r\n",
    "\r\n",
    "    test_loss /= num_batches\r\n",
    "    correct /= size\r\n",
    "    #print(f\"Test Metrics: \\n Test Accuracy: {(100*correct):>0.1f}%, Test Loss: {test_loss:>8f} \\n\")\r\n",
    "    print(f\"Test Metrics: \\n Test Accuracy: {(100*acc):>0.1f}%, Test Loss: {test_loss:>8f} \\n\")\r\n",
    "    #wandb.log({\"Test Accuracy\":acc, \"Test Loss\": test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\r\n",
    "\r\n",
    "#loss_fn = nn.NLLLoss()\r\n",
    "loss_fn = config.get(\"loss\")\r\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=config.get(\"learning_rate\"))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">graceful-water-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/markgich/demo_wandb_fashionmnsit_test\" target=\"_blank\">https://wandb.ai/markgich/demo_wandb_fashionmnsit_test</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/markgich/demo_wandb_fashionmnsit_test/runs/3s7yg1c2\" target=\"_blank\">https://wandb.ai/markgich/demo_wandb_fashionmnsit_test/runs/3s7yg1c2</a><br/>\n",
       "                Run data is saved locally in <code>c:\\Users\\markg\\Documents\\PROGRAMMING\\PYTHON_PROJECTS\\TORCH\\DEMO_WINE_WANDB\\wandb\\run-20210710_160611-3s7yg1c2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "train loss: 2.306896  [    0/60000]\n",
      "train loss: 1.295077  [ 6400/60000]\n",
      "train loss: 1.046859  [12800/60000]\n",
      "train loss: 1.190677  [19200/60000]\n",
      "train loss: 1.211662  [25600/60000]\n",
      "train loss: 1.217390  [32000/60000]\n",
      "train loss: 0.919782  [38400/60000]\n",
      "train loss: 1.175587  [44800/60000]\n",
      "train loss: 1.001915  [51200/60000]\n",
      "train loss: 1.105289  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 65.6%, Train Loss: 1.194652 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 75.0%, Test Loss: 1.008298 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "train loss: 0.871998  [    0/60000]\n",
      "train loss: 0.976365  [ 6400/60000]\n",
      "train loss: 0.839523  [12800/60000]\n",
      "train loss: 1.041528  [19200/60000]\n",
      "train loss: 1.043180  [25600/60000]\n",
      "train loss: 1.157210  [32000/60000]\n",
      "train loss: 0.805272  [38400/60000]\n",
      "train loss: 1.095513  [44800/60000]\n",
      "train loss: 0.954295  [51200/60000]\n",
      "train loss: 1.027219  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.142550 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 68.8%, Test Loss: 0.957108 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "train loss: 0.772252  [    0/60000]\n",
      "train loss: 0.887964  [ 6400/60000]\n",
      "train loss: 0.831834  [12800/60000]\n",
      "train loss: 0.951301  [19200/60000]\n",
      "train loss: 1.094394  [25600/60000]\n",
      "train loss: 1.101803  [32000/60000]\n",
      "train loss: 0.801561  [38400/60000]\n",
      "train loss: 1.044568  [44800/60000]\n",
      "train loss: 0.929823  [51200/60000]\n",
      "train loss: 1.014551  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.104120 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 68.8%, Test Loss: 0.932950 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "train loss: 0.731811  [    0/60000]\n",
      "train loss: 0.880551  [ 6400/60000]\n",
      "train loss: 0.838069  [12800/60000]\n",
      "train loss: 0.902115  [19200/60000]\n",
      "train loss: 1.011955  [25600/60000]\n",
      "train loss: 1.075469  [32000/60000]\n",
      "train loss: 0.784194  [38400/60000]\n",
      "train loss: 1.132665  [44800/60000]\n",
      "train loss: 0.915797  [51200/60000]\n",
      "train loss: 0.986618  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.014906 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 68.8%, Test Loss: 0.915009 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "train loss: 0.707029  [    0/60000]\n",
      "train loss: 0.861363  [ 6400/60000]\n",
      "train loss: 0.793423  [12800/60000]\n",
      "train loss: 0.962098  [19200/60000]\n",
      "train loss: 0.925474  [25600/60000]\n",
      "train loss: 1.052532  [32000/60000]\n",
      "train loss: 0.741855  [38400/60000]\n",
      "train loss: 1.124134  [44800/60000]\n",
      "train loss: 0.915139  [51200/60000]\n",
      "train loss: 0.980870  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.050480 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 75.0%, Test Loss: 0.905584 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "train loss: 0.693409  [    0/60000]\n",
      "train loss: 0.848706  [ 6400/60000]\n",
      "train loss: 0.765144  [12800/60000]\n",
      "train loss: 0.947968  [19200/60000]\n",
      "train loss: 0.899328  [25600/60000]\n",
      "train loss: 1.113926  [32000/60000]\n",
      "train loss: 0.738086  [38400/60000]\n",
      "train loss: 1.097872  [44800/60000]\n",
      "train loss: 0.916241  [51200/60000]\n",
      "train loss: 1.022677  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.020657 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 75.0%, Test Loss: 0.896831 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "train loss: 0.626140  [    0/60000]\n",
      "train loss: 0.829966  [ 6400/60000]\n",
      "train loss: 0.790279  [12800/60000]\n",
      "train loss: 0.920210  [19200/60000]\n",
      "train loss: 0.935394  [25600/60000]\n",
      "train loss: 1.057129  [32000/60000]\n",
      "train loss: 0.694642  [38400/60000]\n",
      "train loss: 1.098832  [44800/60000]\n",
      "train loss: 0.910753  [51200/60000]\n",
      "train loss: 1.026932  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 1.020325 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 75.0%, Test Loss: 0.888974 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "train loss: 0.659439  [    0/60000]\n",
      "train loss: 0.827340  [ 6400/60000]\n",
      "train loss: 0.735483  [12800/60000]\n",
      "train loss: 0.905411  [19200/60000]\n",
      "train loss: 0.871718  [25600/60000]\n",
      "train loss: 1.032517  [32000/60000]\n",
      "train loss: 0.713951  [38400/60000]\n",
      "train loss: 1.065778  [44800/60000]\n",
      "train loss: 0.916270  [51200/60000]\n",
      "train loss: 1.004468  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 0.920053 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 68.8%, Test Loss: 0.881256 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "train loss: 0.661119  [    0/60000]\n",
      "train loss: 0.818130  [ 6400/60000]\n",
      "train loss: 0.704518  [12800/60000]\n",
      "train loss: 0.894348  [19200/60000]\n",
      "train loss: 0.920537  [25600/60000]\n",
      "train loss: 1.018299  [32000/60000]\n",
      "train loss: 0.686907  [38400/60000]\n",
      "train loss: 1.037310  [44800/60000]\n",
      "train loss: 0.897116  [51200/60000]\n",
      "train loss: 0.976563  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 65.6%, Train Loss: 1.038804 \n",
      "\n",
      "Test Metrics: \n",
      " Test Accuracy: 81.2%, Test Loss: 0.878120 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "train loss: 0.621192  [    0/60000]\n",
      "train loss: 0.834126  [ 6400/60000]\n",
      "train loss: 0.745424  [12800/60000]\n",
      "train loss: 0.913020  [19200/60000]\n",
      "train loss: 0.901582  [25600/60000]\n",
      "train loss: 1.046606  [32000/60000]\n",
      "train loss: 0.687902  [38400/60000]\n",
      "train loss: 1.091007  [44800/60000]\n",
      "train loss: 0.874346  [51200/60000]\n",
      "train loss: 0.970084  [57600/60000]\n",
      "Train Metrics: \n",
      " Train Accuracy: 62.5%, Train Loss: 0.958502 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: \n",
      " Test Accuracy: 68.8%, Test Loss: 0.871815 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36012<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d06f0ea605467fb20684088619f55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>c:\\Users\\markg\\Documents\\PROGRAMMING\\PYTHON_PROJECTS\\TORCH\\DEMO_WINE_WANDB\\wandb\\run-20210710_160611-3s7yg1c2\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>c:\\Users\\markg\\Documents\\PROGRAMMING\\PYTHON_PROJECTS\\TORCH\\DEMO_WINE_WANDB\\wandb\\run-20210710_160611-3s7yg1c2\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Accuracy</td><td>0.625</td></tr><tr><td>Train Loss</td><td>0.9585</td></tr><tr><td>_runtime</td><td>133</td></tr><tr><td>_timestamp</td><td>1625922504</td></tr><tr><td>_step</td><td>19</td></tr><tr><td>Test Accuracy</td><td>0.6875</td></tr><tr><td>Test Loss</td><td>136.87503</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Accuracy</td><td>█▁▁▁▁▁▁▁█▁</td></tr><tr><td>Train Loss</td><td>█▇▆▃▄▄▄▁▄▂</td></tr><tr><td>_runtime</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Test Accuracy</td><td>▅▁▁▁▅▅▅▁█▁</td></tr><tr><td>Test Loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">graceful-water-12</strong>: <a href=\"https://wandb.ai/markgich/demo_wandb_fashionmnsit_test/runs/3s7yg1c2\" target=\"_blank\">https://wandb.ai/markgich/demo_wandb_fashionmnsit_test/runs/3s7yg1c2</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "\r\n",
    "with wandb.init(project=\"demo_wandb_fashionmnsit_test\", config=config):\r\n",
    "    wandb.watch(model, criterion=loss_fn, log=\"all\", log_freq=10)\r\n",
    "    epochs = 10\r\n",
    "    for t in range(epochs):\r\n",
    "\r\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\r\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\r\n",
    "        test_loop(test_dataloader, model, loss_fn)\r\n",
    "    wandb.save(torch.save(model.state_dict(), f=f\"./models/fashion_mnist_{t}.pt\"))\r\n",
    "print(\"Done!\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "778de6fd147bda85163a324ce7339acfc1c656b45f8b8593316159881a238432"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('torch-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}